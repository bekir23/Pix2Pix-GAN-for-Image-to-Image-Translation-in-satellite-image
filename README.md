# Pix2Pix-GAN-for-Image-to-Image-Translation-in-satellite-image

The Pix2Pix GAN is a general approach for image-to-image translation.<br/>
It is based on the conditional generative adversarial network, where a target image is generated, conditional on a given input image.<br/>

Dataset that used for this project  comprised of satellite images of New York and their corresponding Google maps pages. <br/>
The image translation problem involves converting satellite photos to Google maps format, or the reverse, Google maps images to Satellite photos.<br/>

The dataset is provided on the pix2pix website and can be downloaded as a 255-megabyte zip file.<br/>

https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/

Our result can seen below. First row indicate input image, Second row model's indicate predict. Last row indicate ground truth<br/>
![alt text](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/Plot-of-Satellite-to-Google-Map-Translated-Images-Using-Pix2Pix-After-100-Training-Epochs.png)
